{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import MultivariateNormal, Normal\n",
    "from torch.optim.lr_scheduler import LambdaLR, ConstantLR, CyclicLR\n",
    "from torch.autograd import Variable\n",
    "from scipy.optimize import root_scalar, shgo\n",
    "\n",
    "device = \"cpu\"\n",
    "default_tensor_type = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340e808",
   "metadata": {},
   "source": [
    "# Problem Setting\n",
    "\n",
    "The Objective of this notebook is to study the compromise Performance vs Robustness of Statistics over distributions of Rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be497ff",
   "metadata": {},
   "source": [
    "We need to define an indexation order on $\\mathfrak{S}_n$ in order to write the loss as a matrix in $\\mathbb{R}^{n! \\times n!}$ and distributions as vectors in $\\mathbb{R}^{n!}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = 4\n",
    "all_ranks = list(itertools.permutations(list(np.arange(n_items))))\n",
    "all_ranks = [np.array(elem) for elem in all_ranks]\n",
    "torch_all_ranks = torch.from_numpy(np.asarray(all_ranks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ff1fc",
   "metadata": {},
   "source": [
    "**DEFINITION:** A statistics over a distribution of rankings is a function $T:\\Delta^{\\mathfrak{S}_n} \\to \\mathfrak{S}_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243e89b",
   "metadata": {},
   "source": [
    "**PERFORMANCE** as defined by \n",
    "$$\n",
    "p \\mapsto \\mathbb{E}_{Y\\sim p}\\left(\\tau_K(Y, T(p))\\right)\n",
    "$$\n",
    "where \n",
    "$\\tau_K$ is the Kendall Tau:\n",
    "$$\n",
    "\\tau_K(y, \\sigma) = \\sum_{i,j} \\mathbb{1}\\{\\sigma^{-1}(i) < \\sigma^{-1}(j)\\}\\mathbb{1}\\{y^{-1}(i) > y^{-1}(j)\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c69ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_matrix(p_torch, torch_all_ranks, n=4):\n",
    "    M = torch.zeros(n,n)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            idxs = torch.tensor([torch.argwhere(rk == i).item() < torch.argwhere(rk == j).item() for rk in torch_all_ranks])\n",
    "            val = torch.sum(p_torch[0,idxs]).item()\n",
    "            M[i,j] = val\n",
    "            M[j,i] = 1-val\n",
    "    return M\n",
    "\n",
    "\n",
    "def expected_kendall(P1, P2):\n",
    "    return torch.norm(P1 * (1-P2), 1)\n",
    "    #return torch.sum(torch.triu(P1,1)*torch.triu(1-P2,1) + torch.triu(P2,1)*torch.triu(1-P1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89fc22c",
   "metadata": {},
   "source": [
    "**ROBUSTNESS** as defined by the \"breakdown function\" of T:\n",
    "$$\n",
    "\\varepsilon^*(\\delta,p,T) = \\inf \\left\\{\\varepsilon>0 : \\sup_{||p-q|| \\leq \\varepsilon} \\rho_{\\tau_K}(T(p), T(q)) \\geq \\delta \\right\\}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\rho_{\\tau_K} = H_K^{(1/2)}(\\pi_1, \\pi_2) := \\frac{1}{2}\\left(\\max_{\\sigma_1 \\in \\pi_1} \\min_{\\sigma_2 \\in \\pi_2} \\tau_K(\\sigma_1, \\sigma_2) + \\max_{\\sigma_2 \\in \\pi_2} \\min_{\\sigma_1 \\in \\pi_1} \\tau_K(\\sigma_1, \\sigma_2) \\right)\n",
    "$$\n",
    "The difficulty with the computation of the breakdown function is that the underlying optimization cannot be performed directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c52fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrized_hausdorff_on_kendall(P1, P2):\n",
    "    return torch.norm(torch.triu(P1,1) - torch.triu(P2,1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e99a91",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{\\rho}_{\\tau_K} = H_K^{dis}(\\pi_1, \\pi_2) := \\max_{\\sigma_2 \\in \\pi_2} \\min_{\\sigma_1 \\in \\pi_1} \\tau_K(\\sigma_1, \\sigma_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a66c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disymmetrized_hausdorff_on_kendall(P1, P2):\n",
    "    # If P2 is included in P1\n",
    "    if (P1[P1 != 1/2] == P2[P1 != 1/2]).all():\n",
    "        return torch.tensor(0.0)\n",
    "    # If P1 is included in P2\n",
    "    elif (P1[P2 != 1/2] == P2[P2 != 1/2]).all():\n",
    "        return 2*torch.norm(torch.triu(P1,1) - torch.triu(P2,1), 1)\n",
    "    else:\n",
    "        idxs = torch.argwhere(P1 == 1/2)\n",
    "        v = torch.sum(P2 == 1/2)/2\n",
    "        idxs2 = torch.argwhere(P2 == 1/2)\n",
    "        P1[[idxs[:, 0], idxs[:, 1]]] = 0\n",
    "        P2[[idxs[:, 0], idxs[:, 1]]] = 0\n",
    "        P1[[idxs2[:, 0], idxs2[:, 1]]] = 0\n",
    "        P2[[idxs2[:, 0], idxs2[:, 1]]] = 0\n",
    "        return torch.norm(torch.triu(P1,1) - torch.triu(P2,1), 1) + v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25085440",
   "metadata": {},
   "source": [
    "# Computing the breakdown function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d66cc50",
   "metadata": {},
   "source": [
    "We approximate the computation of $\\varepsilon^*$ in the following way:\n",
    "\n",
    "1. We find a distribution $\\tilde q_{\\varepsilon,\\delta, p, T}$ that is a solution of the smoothed problem:\n",
    "$$\n",
    "\\tilde q_{\\varepsilon, p, T} = \\arg\\sup_{||p-q|| \\leq \\varepsilon} \\phi(p,q)\n",
    "$$\n",
    "where $\\phi(\\cdot,q) = \\rho_{\\tau_K}(T(\\cdot), T(q)) \\star k(\\cdot)$ with $k$ being a smoothing convolution kernel, here a multivariate Gaussian.\n",
    "2. We approximate $\\varepsilon^*$ by\n",
    "$$\n",
    "\\tilde\\varepsilon(\\delta,p,T) = \\inf \\left\\{\\varepsilon>0 : \\rho_{\\tau_K}\\left(T(p), T(\\tilde q_{\\varepsilon, p, T})\\right) \\geq \\delta \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9a7e92",
   "metadata": {},
   "source": [
    "### Computing $\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a435f9",
   "metadata": {},
   "source": [
    "A method for smoothing a bivariate function $m:{\\cal Y}\\times {\\cal X} \\to \\mathbb{R}$ on its second argument using a mean convolution with kernel $k$.\n",
    "\n",
    "$$\n",
    "\\tilde m(y, x) = (m(y,\\cdot)\\star k)(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef0dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_pg_2(m, k_x):\n",
    "    r\"\"\"\n",
    "    Smooth a metric $m$ by convolution with a kernel $k_x$ centered in $x$.\n",
    "    $$\\tilde{m}(y,x) = \\int m(y,u) k_x(u) du$$\n",
    "    As the gradient is not available in closed-form, the loss build is a policy gradient loss, that leads to a noisy but\n",
    "    unbiased estimate of the gradient of $\\tilde{m}$.\n",
    "    $$g_x(y,u) = \\log(k_x(u)) f(y,u) ~~~~\\text{for}~~~ u\\sim k_x$$\n",
    "    :param m: function to be smoothed\n",
    "    :param k_x: function $x\\mapsto k$ where $k$ is a kernel centered in $x$\n",
    "    :return: $g_x(y,u)$ for $u\\sim k_x$\n",
    "    \"\"\"\n",
    "    def smoothed_m_pg(y, x):\n",
    "        k = k_x(x)\n",
    "        batch_size = torch.Size()\n",
    "        if len(y.size()) > 1:\n",
    "            batch_size = y.size()[:-1]\n",
    "        u = k.sample(batch_size).to(device).type(default_tensor_type)\n",
    "        loss = k.log_prob(u) * m(y,u)\n",
    "        return loss\n",
    "    \n",
    "    return smoothed_m_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deadb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_phi(p, q, dist_Tp_Tq, std_dev_=0.00001, nb_simu=50):\n",
    "    kernel_conv = lambda _s: MultivariateNormal(_s, std_dev_*torch.eye(_s.size()[-1]))\n",
    "    rhos = list()\n",
    "    for i in range(nb_simu):\n",
    "        q2 = kernel_conv(q.float()).sample(torch.Size())\n",
    "        rho = dist_Tp_Tq(p, q2.unsqueeze(0)).detach().numpy().item()\n",
    "        rhos.append(rho)\n",
    "    rho_final = np.mean(rhos)\n",
    "    #std_rho = np.std(rhos)\n",
    "    #print(f\"Monte carlo phi: std = {std_rho}\")\n",
    "    return torch.tensor(rho_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7eeec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewPhi(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = \"New Phi class\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_q2, input_backward, p_torch, dist_Tp_Tq, std_dev_, nb_simu):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input_backward)\n",
    "        monte_carlo_val = monte_carlo_phi(p_torch, input_q2, dist_Tp_Tq, std_dev_=std_dev_, nb_simu=nb_simu)\n",
    "        return monte_carlo_val\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        back_,  = ctx.saved_tensors\n",
    "        return back_ * grad_output, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.rand([24], requires_grad=True)\n",
    "q = softplus(s)/torch.sum(softplus(s))\n",
    "print(q)\n",
    "\n",
    "kernel_conv = lambda _s: MultivariateNormal(_s, 1.0*torch.eye(_s.size()[-1]))\n",
    "\n",
    "smooth_pg_2(dist_Tp_Tq, kernel_conv)(p_torch, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c260c8bd",
   "metadata": {},
   "source": [
    "### Computing $\\tilde q_{\\varepsilon,\\delta, p, T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedef9ca",
   "metadata": {},
   "source": [
    "Remember we want to compute\n",
    "$$\n",
    "\\tilde q_{\\varepsilon, p, T} = \\arg\\sup_{||p-q|| \\leq \\varepsilon} \\phi(p,q)\n",
    "$$\n",
    "Practically, we solved the Lagrangian relaxation of this\n",
    "$$\n",
    "\\tilde q_{\\lambda, p, T} = \\arg\\sup_q \\phi(p,q) - \\lambda ||p-q||\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde64dd2",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "\n",
    "Here we use \"smooth_pg_2\", which approximates THE GRADIENT of phi, and then we call backward on that --> redondancy ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_optim_attack_relaxed(dist_Tp_Tq, p_torch, reg, epochs, std_dev=0.01, lr=0.01):\n",
    "    r\"\"\"\n",
    "    :param method: $\\tau_K(T(p), T(q))$\n",
    "    :param reg: $\\lambda$\n",
    "    :param epochs: limit on the number of optimization iterations\n",
    "    \"\"\"\n",
    "    p_torch2 = p_torch.detach().clone()\n",
    "    p_torch2.requires_grad = False\n",
    "    softplus = torch.nn.Softplus(beta=1, threshold=20)\n",
    "    kernel_conv = lambda _s: MultivariateNormal(_s, std_dev*torch.eye(_s.size()[-1])) # $x\\mapsto k_x(.)$\n",
    "    smoothed_dist_Tp_Tq = lambda p, q: smooth_pg_2(dist_Tp_Tq, kernel_conv)(p, q)     # $\\phi(.,.)$\n",
    "    \n",
    "    s_ = p_torch[0,:].detach().clone().to(device).type(default_tensor_type)\n",
    "    s_.requires_grad = True\n",
    "    optimizer = torch.optim.SGD([s_], lr=0.01, momentum=0.9)\n",
    "    scheduler = CyclicLR(optimizer, 0.01, 1.0, step_size_down=50, cycle_momentum=False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        q = softplus(s_)/torch.sum(softplus(s_))                                   # projection $\\sum = 1$\n",
    "        \n",
    "        # Decrease the approximation error over time\n",
    "        kernel_conv = lambda _s: MultivariateNormal(_s, std_dev*(10/(10+epoch))*torch.eye(_s.size()[-1])) # $x\\mapsto k_x(.)$\n",
    "        smoothed_dist_Tp_Tq = lambda p, q: smooth_pg_2(dist_Tp_Tq, kernel_conv)(p, q)     # $\\phi(.,.)$\n",
    "        \n",
    "        loss = -smoothed_dist_Tp_Tq(p_torch2, q) + reg*(torch.norm(p_torch2-q, 1))     # Lagrangian relaxation of the objective\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efa88f",
   "metadata": {},
   "source": [
    "### Computing $\n",
    "\\tilde\\varepsilon(\\delta,p,T) = \\inf \\left\\{\\varepsilon>0 : \\rho_{\\tau_K}\\left(T(p), T(\\tilde q_{\\varepsilon, p, T})\\right) \\geq \\delta \\right\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7d18b",
   "metadata": {},
   "source": [
    "First strategy: using $\\rho_{\\tau_K}$ in the evaluation of the breakdown point. Cons: piecewise-constant function, unstable results in the end.\n",
    "\n",
    "Second strategy: using $\\phi$ (the smoothed version of $\\rho_{\\tau_K}$). Cons: it seems the results are weird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06732d6",
   "metadata": {},
   "source": [
    "#### Monte-Carlo estimation of rho when using $\\phi$\n",
    "\n",
    "$\\phi(p,q) = \\rho_{\\tau_K}(T(p), T(q)) \\star k(p) = \\int_u \\rho_{\\tau_K}(T(u), T(q)) \\times k(p-u) du = \\int_u \\rho_{\\tau_K}(T(u), T(q)) \\times k_p(u) du = \\mathbb{E}_{p' \\sim k_p}(\\rho_{\\tau_K}(T(p'), T(q)))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal q\n",
    "q = torch_optim_attack_relaxed(dist_Tp_Tq, p_torch, 10, epochs, std_dev=10, lr=0.01)\n",
    "\n",
    "# Real result of rho(T(p), T(q))\n",
    "print(\"Real result:\", torch_dist_maxpair(p_torch, torch.unsqueeze(q,0), torch_all_ranks, threshold=threshold).detach().numpy())\n",
    "\n",
    "# Defining phi\n",
    "std_dev_ = 0.005\n",
    "kernel_conv = lambda _s: MultivariateNormal(_s, std_dev_*torch.eye(_s.size()[-1])) # $x\\mapsto k_x(.)$\n",
    "\n",
    "# A large number of evaluations of phi\n",
    "nb_simu = 10000\n",
    "rhos = list()\n",
    "for i in range(nb_simu):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"simu nb {i}\")\n",
    "    p2 = kernel_conv(p_torch.float().squeeze(0)).sample(torch.Size())\n",
    "    rho = dist_Tp_Tq(p2.unsqueeze(0), q.unsqueeze(0)).detach().numpy().item()\n",
    "    rhos.append(rho)\n",
    "\n",
    "# Mean approximation of phi\n",
    "print(f\"Mean = {np.mean(rhos)}\")\n",
    "\n",
    "# Plot\n",
    "plt.hist(rhos, density=True, bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee94a11",
   "metadata": {},
   "source": [
    "#### Implementation of the function to compute the breakdown point.\n",
    "\n",
    "We want to solve the following pb:\n",
    "\n",
    "$$\\inf_{\\varepsilon \\geq 0} \\varepsilon \\text{ s.t. } \\sup_{||p-q|| \\leq \\varepsilon} \\rho_T(p,q) \\geq \\delta$$\n",
    "\n",
    "This problem is equivalent to the following one, based one Lagrange relaxation:\n",
    "\n",
    "$$ \\inf_{\\varepsilon \\geq 0} \\, \\sup_{\\lambda \\geq 0} \\, \\inf_{q \\in \\Delta} \\, \\sup_{\\alpha \\geq 0} \\, \\, \\varepsilon + \\lambda \\delta - \\lambda \\rho_T(p,q) + \\alpha \\lambda ||p-q|| - \\lambda \\alpha \\varepsilon $$\n",
    "\n",
    "To make it easy to follow, here is what each variable is controling:\n",
    "\n",
    "1) $q$ is the attacking probability vector, which should not be too far from $p$, but change statistics $T$\n",
    "\n",
    "2) $\\varepsilon$ is the attack budget (the breakdown point) controling the perturbation from $q$\n",
    "\n",
    "3) $\\lambda$ controls that the statistics $T$ is indeed broken by at least $\\delta$ by $q$\n",
    "\n",
    "4) $\\alpha$ controls that $q$ is indeed closer than $\\varepsilon$ from $p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_fct(vect, time=10):\n",
    "    l = len(vect)\n",
    "    res = vect[0:l:time]\n",
    "    return res\n",
    "\n",
    "def approximate_breakdown_function(delta, dist_Tp_Tq, p_torch, epochs=150000, std_dev=0.01, lr=0.01, maxiter=10, max_reg=10., eval_strat=\"real\"):\n",
    "    if False:\n",
    "        def _rho_minus_delta(_reg):\n",
    "            q = torch_optim_attack_relaxed(dist_Tp_Tq, p_torch, _reg, epochs, std_dev=std_dev, lr=lr)\n",
    "            if eval_strat == \"real\":\n",
    "                rho = torch_dist_maxpair(p_torch, torch.unsqueeze(q,0), torch_all_ranks, threshold=threshold).detach().numpy()\n",
    "            elif eval_strat == \"smoothed\":\n",
    "                std_dev_ = 0.005\n",
    "                nb_simu = 25000\n",
    "                kernel_conv = lambda _s: MultivariateNormal(_s, std_dev_*torch.eye(_s.size()[-1])) # $x\\mapsto k_x(.)$\n",
    "                rhos = list()\n",
    "                for i in range(nb_simu):\n",
    "                    p2 = kernel_conv(p_torch.float().squeeze(0)).sample(torch.Size())\n",
    "                    rho = dist_Tp_Tq(p2.unsqueeze(0), q.unsqueeze(0)).detach().numpy().item()\n",
    "                    rhos.append(rho)\n",
    "                rho = np.mean(rhos)\n",
    "            else:\n",
    "                print(\"Evaluation strategy not implemented\")\n",
    "            print(f\"rho = {rho} and rho - delta = {rho - delta}\")\n",
    "            return rho - delta\n",
    "\n",
    "        res = root_scalar(_rho_minus_delta, bracket=[1e-8, max_reg], x0=0.5, x1=2.5, xtol=None, rtol=None, maxiter=maxiter)\n",
    "        q = torch_optim_attack_relaxed(dist_Tp_Tq, p_torch, res.root, epochs, std_dev=std_dev, lr=lr)\n",
    "        return torch.norm(p_torch - torch.unsqueeze(q,0),1).detach().numpy()\n",
    "    \n",
    "    else:\n",
    "        #q = torch_optim_attack_relaxed(dist_Tp_Tq, p_torch, epochs=20, std_dev=std_dev, reg=10, lr=0.01)\n",
    "        q2 = p_torch.detach().clone().squeeze(0) #q.detach().clone() #\n",
    "        q2.requires_grad = True\n",
    "        s_ = torch.log(q2)[:].detach().clone().to(device).type(default_tensor_type)\n",
    "        s_.requires_grad = True\n",
    "\n",
    "        epsilon_ = torch.tensor(1.0, requires_grad=True)\n",
    "        eta_ = torch.tensor(10.0, requires_grad=True)\n",
    "        alpha_ = torch.tensor(10.0, requires_grad=True)\n",
    "        qs_ = list()\n",
    "        qs_total_ = list()\n",
    "        epsilons_ = list()\n",
    "        etas_ = list()\n",
    "        alphas_ = list()\n",
    "\n",
    "        mean_epsilons_ = list()\n",
    "        mean_etas_ = list()\n",
    "        mean_alphas_ = list()\n",
    "        mean_qs_ = list()\n",
    "\n",
    "        phis_ = list()\n",
    "        losses = list()\n",
    "\n",
    "        lr_list = list()\n",
    "\n",
    "        optimizer_q2 = torch.optim.Adam([s_], lr=lr, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "        optimizer_eta = torch.optim.Adam([eta_], lr=lr, betas=(0.9, 0.999), weight_decay=0.01, maximize=True)\n",
    "        optimizer_epsilon = torch.optim.Adam([epsilon_], lr=lr, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "        optimizer_alpha = torch.optim.Adam([alpha_], lr=lr, betas=(0.9, 0.999), weight_decay=0.01, maximize=True)\n",
    "\n",
    "\n",
    "        scale_fct_ = lambda x: 0.5**x\n",
    "        scheduler_q2 = torch.optim.lr_scheduler.CyclicLR(optimizer_q2, lr, 10, step_size_up=3000, mode='exp_range', gamma=0.8, cycle_momentum=False, scale_mode='cycle', scale_fn=scale_fct_)\n",
    "        scheduler_eta = torch.optim.lr_scheduler.CyclicLR(optimizer_eta, lr, 10, step_size_up=3000, mode='exp_range', gamma=0.8, cycle_momentum=False, scale_mode='cycle', scale_fn=scale_fct_)\n",
    "        scheduler_epsilon = torch.optim.lr_scheduler.CyclicLR(optimizer_epsilon, lr, 10, step_size_up=3000, mode='exp_range', gamma=0.8, cycle_momentum=False, scale_mode='cycle', scale_fn=scale_fct_)\n",
    "        scheduler_alpha = torch.optim.lr_scheduler.CyclicLR(optimizer_alpha, lr, 10, step_size_up=3000, mode='exp_range', gamma=0.8, cycle_momentum=False, scale_mode='cycle', scale_fn=scale_fct_)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            kernel_conv = lambda _s: MultivariateNormal(_s, 1/np.sqrt(1+epoch)*torch.eye(_s.size()[-1]))\n",
    "            q2 = softplus(s_)/torch.sum(softplus(s_))\n",
    "            res = smooth_pg_2(dist_Tp_Tq, kernel_conv)(p_torch, q2)\n",
    "            res.backward(retain_graph=True)\n",
    "            grad_data = s_.grad.data.detach().clone()\n",
    "            Phi_ = NewPhi.apply\n",
    "            phi_ = Phi_(q2, grad_data, p_torch, dist_Tp_Tq, 1/np.sqrt(1+epoch), 1)\n",
    "\n",
    "            optimizer_q2.zero_grad()\n",
    "            optimizer_eta.zero_grad()\n",
    "            optimizer_epsilon.zero_grad()\n",
    "            optimizer_alpha.zero_grad()\n",
    "\n",
    "            loss = torch.exp(epsilon_) + softplus(eta_)/softplus(alpha_)*delta - softplus(eta_)/softplus(alpha_)*phi_ + softplus(eta_)*torch.norm(p_torch - torch.unsqueeze(q2,0),1) - softplus(eta_)*softplus(epsilon_)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(s_, 1)\n",
    "            torch.nn.utils.clip_grad_value_(epsilon_, 1)\n",
    "            torch.nn.utils.clip_grad_value_(eta_, 1)\n",
    "            torch.nn.utils.clip_grad_value_(alpha_, 1)\n",
    "            optimizer_q2.step()\n",
    "            optimizer_eta.step()\n",
    "            optimizer_epsilon.step()\n",
    "            optimizer_alpha.step()\n",
    "\n",
    "            scheduler_q2.step()\n",
    "            scheduler_eta.step()\n",
    "            scheduler_epsilon.step()\n",
    "            scheduler_alpha.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"epoch {epoch}: \\n \\t epsilon_={softplus(epsilon_)} and grad = {-epsilon_.grad.data} and lr = {optimizer_epsilon.param_groups[0]['lr']} \\n \\t eta_={softplus(eta_)} and grad = {eta_.grad.data} and lr = {optimizer_eta.param_groups[0]['lr']} \\n \\t alpha_={softplus(alpha_)} and grad = {alpha_.grad.data} and lr = {optimizer_alpha.param_groups[0]['lr']} \\n \\t s_={s_}\\n \\t q2={q2} and sum={torch.sum(q2)} and grad = {-s_.grad.data} and lr = {optimizer_q2.param_groups[0]['lr']} \\n \\t phi = {phi_} and std_dev = {1.0/(1+epoch)}\")\n",
    "            losses.append(loss.detach().item())\n",
    "            qs_total_.append(q2.detach().numpy())\n",
    "            qs_.append(q2[0].detach().item())\n",
    "            epsilons_.append(torch.exp(epsilon_).detach().item())\n",
    "            etas_.append(torch.exp(eta_).detach().item())\n",
    "            alphas_.append(torch.exp(alpha_).detach().item())\n",
    "            phis_.append(phi_.item())\n",
    "            lr_list.append(optimizer_epsilon.param_groups[0]['lr'])\n",
    "\n",
    "            mean_qs_.append(np.mean(qs_total_[-50000:], axis=0))\n",
    "            mean_epsilons_.append(np.mean(epsilons_[-50000:]))\n",
    "            mean_etas_.append(np.mean(etas_[-50000:]))\n",
    "            mean_alphas_.append(np.mean(alphas_[-50000:]))\n",
    "            \n",
    "            \n",
    "        f, ax = plt.subplots(2,3, figsize=(15, 8))\n",
    "        ax[0,0].plot(qs_)\n",
    "        ax[0,0].set_title(f\"First value of q\")\n",
    "\n",
    "        ax[0,1].plot(phis_)\n",
    "        ax[0,1].set_title(f\"Phi\")\n",
    "\n",
    "        ax[0,2].plot(epsilons_)\n",
    "        ax[0,2].set_title(f\"Epsilon\")\n",
    "\n",
    "        #ax[1,0].plot(lambdas_)\n",
    "        #ax[1,0].set_title(f\"Lambda\")\n",
    "        ax[1,0].plot(etas_)\n",
    "        ax[1,0].set_title(f\"Eta\")\n",
    "\n",
    "        ax[1,1].plot(alphas_)\n",
    "        ax[1,1].set_title(f\"Alpha\")\n",
    "\n",
    "        ax[1,2].plot(losses)\n",
    "        ax[1,2].set_title(f\"Loss\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n \\n\")\n",
    "        time_fct = 100\n",
    "        x_axis_val = subsample_fct(np.linspace(0,len(mean_epsilons_),len(mean_epsilons_)), time=time_fct)\n",
    "        f, ax = plt.subplots(2,3, figsize=(15, 8))\n",
    "\n",
    "        mean_qs_subsample = subsample_fct(mean_qs_, time_fct)\n",
    "        mean_qs1_ = [torch.norm(p_torch - torch.tensor(mean_q_), 1) for mean_q_ in mean_qs_subsample] #[mean_q_[0] for mean_q_ in mean_qs_]\n",
    "        ax[0,0].plot(x_axis_val, mean_qs1_)\n",
    "        ax[0,0].set_title(f\"Norm of the difference between p and q\")\n",
    "\n",
    "        Phi_ = NewPhi.apply\n",
    "        mean_phis_ = [Phi_(torch.tensor(mean_q_), grad_data, p_torch, dist_Tp_Tq, 0.1/(1+epoch), 10) for mean_q_ in mean_qs_subsample]\n",
    "        ax[0,1].plot(x_axis_val, mean_phis_)\n",
    "        ax[0,1].set_title(f\"Phi\")\n",
    "\n",
    "        ax[0,2].plot(x_axis_val, subsample_fct(mean_epsilons_, time_fct))\n",
    "        ax[0,2].set_title(f\"Epsilon\")\n",
    "\n",
    "        #ax[1,0].plot(x_axis_val, subsample_fct(mean_lambdas_, time_fct))\n",
    "        #ax[1,0].set_title(f\"Lambda\")\n",
    "        ax[1,0].plot(x_axis_val, subsample_fct(mean_etas_, time_fct))\n",
    "        ax[1,0].set_title(f\"Eta\")\n",
    "\n",
    "        ax[1,1].plot(x_axis_val, subsample_fct(mean_alphas_, time_fct))\n",
    "        ax[1,1].set_title(f\"Alpha\")\n",
    "\n",
    "        ax[1,2].plot(x_axis_val, subsample_fct(losses, time_fct))\n",
    "        ax[1,2].set_title(f\"Loss\")\n",
    "        plt.plot()\n",
    "\n",
    "        return losses, qs_, epsilons_, etas_, alphas_, s_, mean_qs_, mean_epsilons_, mean_etas_, mean_alphas_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae3b847",
   "metadata": {},
   "source": [
    "### Warning :\n",
    "\n",
    "You need to compile cells that follows this one (and the 3 following) first (I now, this does not make any sense). Please compile everything under \"Family of distribution\" and \"Statistics under study\" first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142bdf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([25, 8, 3, 2.5])\n",
    "p = proba_plackett_luce(w, all_ranks)\n",
    "p_torch = torch.from_numpy(p)\n",
    "delta = 1\n",
    "threshold = None\n",
    "method_ = erm\n",
    "softplus = torch.nn.Softplus(beta=1, threshold=20)\n",
    "def torch_dist_maxpair(p_torch1, p_torch2, torch_all_ranks, threshold):\n",
    "    R1 = method_(p_torch1, torch_all_ranks, n=4)#, threshold=threshold)\n",
    "    R2 = method_(p_torch2, torch_all_ranks, n=4)#, threshold=threshold)\n",
    "    return symmetrized_hausdorff_on_kendall(R1, R2)\n",
    "\n",
    "method = torch_dist_maxpair\n",
    "dist_Tp_Tq = lambda _p,_q: method(_p, _q, torch_all_ranks, threshold=threshold)\n",
    "\n",
    "#q = torch_optim_attack_relaxed(dist_Tp_Tq, p_torch, 10, 20, std_dev=0.001, lr=0.01)\n",
    "q2 = p_torch.detach().clone().squeeze(0) #q.detach().clone()\n",
    "q2.requires_grad = True\n",
    "s_ = torch.log(q2)[:].detach().clone().to(device).type(default_tensor_type)\n",
    "print(s_.shape)\n",
    "s_.requires_grad = True\n",
    "\n",
    "epsilon_ = torch.tensor(2.0, requires_grad=True)\n",
    "lambda_ = torch.tensor(2.0, requires_grad=True)\n",
    "alpha_ = torch.tensor(3.0, requires_grad=True)\n",
    "qs_ = list()\n",
    "qs_total_ = list()\n",
    "epsilons_ = list()\n",
    "lambdas_ = list()\n",
    "alphas_ = list()\n",
    "\n",
    "mean_epsilons_ = list()\n",
    "mean_lambdas_ = list()\n",
    "mean_alphas_ = list()\n",
    "mean_qs_ = list()\n",
    "\n",
    "phis_ = list()\n",
    "losses = list()\n",
    "\n",
    "optimizer_q2 = torch.optim.Adam([s_], lr=10, betas=(0.9, 0.999), weight_decay=0.001)\n",
    "optimizer_lambda = torch.optim.Adam([lambda_], lr=0.01, betas=(0.9, 0.999), weight_decay=0.001, maximize=True)\n",
    "optimizer_epsilon = torch.optim.Adam([epsilon_], lr=0.1, betas=(0.9, 0.999), weight_decay=0.001)\n",
    "optimizer_alpha = torch.optim.Adam([alpha_], lr=0.01, betas=(0.9, 0.999), weight_decay=0.001, maximize=True)\n",
    "\n",
    "lambda_scheduler = lambda coef: lambda epoch: coef/np.sqrt(1+epoch) if epoch <= 5000 else 1/np.sqrt(1+5000)\n",
    "scheduler_q2 = torch.optim.lr_scheduler.LambdaLR(optimizer_q2, lr_lambda=lambda_scheduler(1)) #.ReduceLROnPlateau(optimizer_q2, patience=10000, factor=0.75, verbose=True, mode=\"max\")\n",
    "scheduler_lambda = torch.optim.lr_scheduler.LambdaLR(optimizer_lambda, lr_lambda=lambda_scheduler(10)) #.ReduceLROnPlateau(optimizer_lambda, patience=10000, factor=0.75, verbose=True)\n",
    "scheduler_epsilon = torch.optim.lr_scheduler.LambdaLR(optimizer_epsilon, lr_lambda=lambda_scheduler(1)) #.ReduceLROnPlateau(optimizer_epsilon, patience=10000, factor=0.75, verbose=True, mode=\"max\")\n",
    "scheduler_alpha = torch.optim.lr_scheduler.LambdaLR(optimizer_alpha, lr_lambda=lambda_scheduler(10)) #.ReduceLROnPlateau(optimizer_alpha, patience=10000, factor=0.75, verbose=True)\n",
    "\n",
    "#scheduler_q2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_q2, patience=50, factor=0.95, verbose=True, mode=\"max\")\n",
    "#scheduler_lambda = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_lambda, patience=50, factor=0.85, verbose=True, mode=\"min\")\n",
    "#scheduler_epsilon = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_epsilon, patience=50, factor=0.95, verbose=True, mode=\"max\")\n",
    "#scheduler_alpha = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_alpha, patience=50, factor=0.5, verbose=True, mode=\"min\")\n",
    "\n",
    "for epoch in range(100):\n",
    "    kernel_conv = lambda _s: MultivariateNormal(_s, 1/np.sqrt(1+epoch)*torch.eye(_s.size()[-1]))\n",
    "    q2 = softplus(s_)/torch.sum(softplus(s_))\n",
    "    res = smooth_pg_2(dist_Tp_Tq, kernel_conv)(p_torch, q2)\n",
    "    res.backward(retain_graph=True)\n",
    "    grad_data = s_.grad.data.detach().clone()\n",
    "    Phi_ = NewPhi.apply\n",
    "    phi_ = Phi_(q2, grad_data, p_torch, dist_Tp_Tq, 0.1/(1+epoch), 3)\n",
    "    \n",
    "    optimizer_q2.zero_grad()\n",
    "    optimizer_lambda.zero_grad()\n",
    "    optimizer_epsilon.zero_grad()\n",
    "    optimizer_alpha.zero_grad()\n",
    "    \n",
    "    loss = torch.exp(epsilon_) + softplus(lambda_)*delta - softplus(lambda_)*phi_ + softplus(lambda_)*softplus(alpha_)*torch.norm(p_torch - torch.unsqueeze(q2,0),1) - softplus(lambda_)*softplus(alpha_)*softplus(epsilon_)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer_q2.step()\n",
    "    optimizer_lambda.step()\n",
    "    optimizer_epsilon.step()\n",
    "    optimizer_alpha.step()\n",
    "    \n",
    "    scheduler_q2.step()#(loss.detach().item())\n",
    "    scheduler_lambda.step()#(loss.detach().item())\n",
    "    scheduler_epsilon.step()#(loss.detach().item())\n",
    "    scheduler_alpha.step()#(loss.detach().item())\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch}: \\n \\t epsilon_={torch.exp(epsilon_)} and grad = {-epsilon_.grad.data} and lr = {optimizer_epsilon.param_groups[0]['lr']} \\n \\t lambda_={torch.exp(lambda_)} and grad = {lambda_.grad.data} and lr = {optimizer_lambda.param_groups[0]['lr']} \\n \\t alpha_={torch.exp(alpha_)} and grad = {alpha_.grad.data} and lr = {optimizer_alpha.param_groups[0]['lr']} \\n \\t s_={s_}\\n \\t q2={q2} and sum={torch.sum(q2)} and grad = {-s_.grad.data} and lr = {optimizer_q2.param_groups[0]['lr']} \\n \\t phi = {phi_} and std_dev = {1.0/(1+epoch)}\")\n",
    "    losses.append(loss.detach().item())\n",
    "    qs_total_.append(q2.detach().numpy())\n",
    "    qs_.append(q2[0].detach().item())\n",
    "    epsilons_.append(torch.exp(epsilon_).detach().item())\n",
    "    lambdas_.append(torch.exp(lambda_).detach().item())\n",
    "    alphas_.append(torch.exp(alpha_).detach().item())\n",
    "    phis_.append(phi_.item())\n",
    "    \n",
    "    mean_qs_.append(np.mean(qs_total_, axis=0))\n",
    "    mean_epsilons_.append(np.mean(epsilons_))\n",
    "    mean_lambdas_.append(np.mean(lambdas_))\n",
    "    mean_alphas_.append(np.mean(alphas_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([25, 8, 3, 2.5])\n",
    "p = proba_plackett_luce(w, all_ranks)\n",
    "p_torch = torch.from_numpy(p)\n",
    "delta = 1\n",
    "threshold = None\n",
    "method_ = erm\n",
    "softplus = torch.nn.Softplus(beta=1, threshold=20)\n",
    "def torch_dist_maxpair(p_torch1, p_torch2, torch_all_ranks, threshold):\n",
    "    R1 = method_(p_torch1, torch_all_ranks, n=4)#, threshold=threshold)\n",
    "    R2 = method_(p_torch2, torch_all_ranks, n=4)#, threshold=threshold)\n",
    "    return symmetrized_hausdorff_on_kendall(R1, R2)\n",
    "\n",
    "method = torch_dist_maxpair\n",
    "dist_Tp_Tq = lambda _p,_q: method(_p, _q, torch_all_ranks, threshold=threshold)\n",
    "\n",
    "#q = torch_optim_attack_relaxed(dist_Tp_Tq, p_torch, 10, 20, std_dev=0.001, lr=0.01)\n",
    "q2 = p_torch.detach().clone().squeeze(0) #q.detach().clone() #\n",
    "q2.requires_grad = True\n",
    "s_ = torch.log(q2)[:].detach().clone().to(device).type(default_tensor_type)\n",
    "print(s_.shape)\n",
    "s_.requires_grad = True\n",
    "\n",
    "epsilon_ = torch.tensor(1.0, requires_grad=True)\n",
    "eta_ = torch.tensor(10.0, requires_grad=True)\n",
    "alpha_ = torch.tensor(10.0, requires_grad=True)\n",
    "qs_ = list()\n",
    "qs_total_ = list()\n",
    "epsilons_ = list()\n",
    "etas_ = list()\n",
    "alphas_ = list()\n",
    "\n",
    "mean_epsilons_ = list()\n",
    "mean_etas_ = list()\n",
    "mean_alphas_ = list()\n",
    "mean_qs_ = list()\n",
    "\n",
    "phis_ = list()\n",
    "losses = list()\n",
    "\n",
    "lr_list = list()\n",
    "\n",
    "optimizer_q2 = torch.optim.Adam([s_], lr=0.01, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "optimizer_eta = torch.optim.Adam([eta_], lr=0.01, betas=(0.9, 0.999), weight_decay=0.01, maximize=True)\n",
    "optimizer_epsilon = torch.optim.Adam([epsilon_], lr=0.01, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "optimizer_alpha = torch.optim.Adam([alpha_], lr=0.01, betas=(0.9, 0.999), weight_decay=0.01, maximize=True)\n",
    "\n",
    "\n",
    "scale_fct_ = lambda x: 0.5**x\n",
    "scheduler_q2 = torch.optim.lr_scheduler.CyclicLR(optimizer_q2, 0.01, 50, step_size_up=3000, mode='exp_range', gamma=0.8, cycle_momentum=False, scale_mode='cycle', scale_fn=scale_fct_)\n",
    "scheduler_eta = torch.optim.lr_scheduler.CyclicLR(optimizer_eta, 0.01, 50, step_size_up=3000, mode='exp_range', gamma=0.8, cycle_momentum=False, scale_mode='cycle', scale_fn=scale_fct_)\n",
    "scheduler_epsilon = torch.optim.lr_scheduler.CyclicLR(optimizer_epsilon, 0.01, 50, step_size_up=3000, mode='exp_range', gamma=0.8, cycle_momentum=False, scale_mode='cycle', scale_fn=scale_fct_)\n",
    "scheduler_alpha = torch.optim.lr_scheduler.CyclicLR(optimizer_alpha, 0.01, 50, step_size_up=3000, mode='exp_range', gamma=0.8, cycle_momentum=False, scale_mode='cycle', scale_fn=scale_fct_)\n",
    "\n",
    "#lambda_scheduler = lambda coef: lambda epoch: coef/np.sqrt(1+epoch) if epoch <= 25000 else 1/np.sqrt(1+25000)\n",
    "#scheduler_q2 = torch.optim.lr_scheduler.LambdaLR(optimizer_q2, lr_lambda=lambda_scheduler(1)) #.ReduceLROnPlateau(optimizer_q2, patience=10000, factor=0.75, verbose=True, mode=\"max\")\n",
    "#scheduler_eta = torch.optim.lr_scheduler.LambdaLR(optimizer_eta, lr_lambda=lambda_scheduler(1)) #.ReduceLROnPlateau(optimizer_lambda, patience=10000, factor=0.75, verbose=True)\n",
    "#scheduler_epsilon = torch.optim.lr_scheduler.LambdaLR(optimizer_epsilon, lr_lambda=lambda_scheduler(1)) #.ReduceLROnPlateau(optimizer_epsilon, patience=10000, factor=0.75, verbose=True, mode=\"max\")\n",
    "#scheduler_alpha = torch.optim.lr_scheduler.LambdaLR(optimizer_alpha, lr_lambda=lambda_scheduler(1)) #.ReduceLROnPlateau(optimizer_alpha, patience=10000, factor=0.75, verbose=True)\n",
    "\n",
    "#scheduler_q2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_q2, patience=50, factor=0.95, verbose=True, mode=\"max\")\n",
    "#scheduler_lambda = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_lambda, patience=50, factor=0.85, verbose=True, mode=\"min\")\n",
    "#scheduler_epsilon = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_epsilon, patience=50, factor=0.95, verbose=True, mode=\"max\")\n",
    "#scheduler_alpha = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_alpha, patience=50, factor=0.5, verbose=True, mode=\"min\")\n",
    "\n",
    "for epoch in range(100):\n",
    "    kernel_conv = lambda _s: MultivariateNormal(_s, 1/np.sqrt(1+epoch)*torch.eye(_s.size()[-1]))\n",
    "    q2 = softplus(s_)/torch.sum(softplus(s_))\n",
    "    res = smooth_pg_2(dist_Tp_Tq, kernel_conv)(p_torch, q2)\n",
    "    res.backward(retain_graph=True)\n",
    "    grad_data = s_.grad.data.detach().clone()\n",
    "    Phi_ = NewPhi.apply\n",
    "    phi_ = Phi_(q2, grad_data, p_torch, dist_Tp_Tq, 1/np.sqrt(1+epoch), 1)\n",
    "    \n",
    "    optimizer_q2.zero_grad()\n",
    "    optimizer_eta.zero_grad()\n",
    "    optimizer_epsilon.zero_grad()\n",
    "    optimizer_alpha.zero_grad()\n",
    "    \n",
    "    loss = torch.exp(epsilon_) + softplus(eta_)/softplus(alpha_)*delta - softplus(eta_)/softplus(alpha_)*phi_ + softplus(eta_)*torch.norm(p_torch - torch.unsqueeze(q2,0),1) - softplus(eta_)*softplus(epsilon_)\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(s_, 1)\n",
    "    torch.nn.utils.clip_grad_value_(epsilon_, 1)\n",
    "    torch.nn.utils.clip_grad_value_(eta_, 1)\n",
    "    torch.nn.utils.clip_grad_value_(alpha_, 1)\n",
    "    optimizer_q2.step()\n",
    "    optimizer_eta.step()\n",
    "    optimizer_epsilon.step()\n",
    "    optimizer_alpha.step()\n",
    "    \n",
    "    scheduler_q2.step()#(loss.detach().item())\n",
    "    scheduler_eta.step()#(loss.detach().item())\n",
    "    scheduler_epsilon.step()#(loss.detach().item())\n",
    "    scheduler_alpha.step()#(loss.detach().item())\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch}: \\n \\t epsilon_={softplus(epsilon_)} and grad = {-epsilon_.grad.data} and lr = {optimizer_epsilon.param_groups[0]['lr']} \\n \\t eta_={softplus(eta_)} and grad = {eta_.grad.data} and lr = {optimizer_eta.param_groups[0]['lr']} \\n \\t alpha_={softplus(alpha_)} and grad = {alpha_.grad.data} and lr = {optimizer_alpha.param_groups[0]['lr']} \\n \\t s_={s_}\\n \\t q2={q2} and sum={torch.sum(q2)} and grad = {-s_.grad.data} and lr = {optimizer_q2.param_groups[0]['lr']} \\n \\t phi = {phi_} and std_dev = {1.0/(1+epoch)}\")\n",
    "    losses.append(loss.detach().item())\n",
    "    qs_total_.append(q2.detach().numpy())\n",
    "    qs_.append(q2[0].detach().item())\n",
    "    epsilons_.append(torch.exp(epsilon_).detach().item())\n",
    "    etas_.append(torch.exp(eta_).detach().item())\n",
    "    alphas_.append(torch.exp(alpha_).detach().item())\n",
    "    phis_.append(phi_.item())\n",
    "    lr_list.append(optimizer_epsilon.param_groups[0]['lr'])\n",
    "    \n",
    "    mean_qs_.append(np.mean(qs_total_[-50000:], axis=0))\n",
    "    mean_epsilons_.append(np.mean(epsilons_[-50000:]))\n",
    "    mean_etas_.append(np.mean(etas_[-50000:]))\n",
    "    mean_alphas_.append(np.mean(alphas_[-50000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epsilons_[-1])\n",
    "print(etas_[-1])\n",
    "#print(lambdas_[-1])\n",
    "print(alphas_[-1])\n",
    "print(qs_[-1])\n",
    "\n",
    "plt.plot(lr_list)\n",
    "plt.show()\n",
    "lr_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2,3, figsize=(15, 8))\n",
    "\n",
    "l_ = 10000\n",
    "\n",
    "ax[0,0].plot(qs_[-l_:])\n",
    "ax[0,0].set_title(f\"First value of q\")\n",
    "\n",
    "ax[0,1].plot(phis_[-l_:])\n",
    "ax[0,1].set_title(f\"Phi\")\n",
    "\n",
    "ax[0,2].plot(epsilons_[-l_:])\n",
    "ax[0,2].set_title(f\"Epsilon\")\n",
    "\n",
    "#ax[1,0].plot(lambdas_)\n",
    "#ax[1,0].set_title(f\"Lambda\")\n",
    "ax[1,0].plot(etas_[-l_:])\n",
    "ax[1,0].set_title(f\"Eta\")\n",
    "\n",
    "ax[1,1].plot(alphas_[-l_:])\n",
    "ax[1,1].set_title(f\"Alpha\")\n",
    "\n",
    "ax[1,2].plot(losses[-l_:])\n",
    "ax[1,2].set_title(f\"Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_fct(vect, time=10):\n",
    "    l = len(vect)\n",
    "    res = vect[0:l:time]\n",
    "    return res\n",
    "time_fct = 10\n",
    "\n",
    "x_axis_val = subsample_fct(np.linspace(0,len(mean_epsilons_),len(mean_epsilons_)), time=time_fct)\n",
    "\n",
    "f, ax = plt.subplots(2,3, figsize=(15, 8))\n",
    "\n",
    "mean_qs_subsample = subsample_fct(mean_qs_, time_fct)\n",
    "mean_qs1_ = [torch.norm(p_torch - torch.tensor(mean_q_), 1) for mean_q_ in mean_qs_subsample] #[mean_q_[0] for mean_q_ in mean_qs_]\n",
    "ax[0,0].plot(x_axis_val, mean_qs1_)\n",
    "ax[0,0].set_title(f\"Norm of the difference between p and q\")\n",
    "\n",
    "Phi_ = NewPhi.apply\n",
    "mean_phis_ = [Phi_(torch.tensor(mean_q_), grad_data, p_torch, dist_Tp_Tq, 0.1/(1+epoch), 10) for mean_q_ in mean_qs_subsample]\n",
    "ax[0,1].plot(x_axis_val, mean_phis_)\n",
    "ax[0,1].set_title(f\"Phi\")\n",
    "\n",
    "ax[0,2].plot(x_axis_val, subsample_fct(mean_epsilons_, time_fct))\n",
    "ax[0,2].set_title(f\"Epsilon\")\n",
    "\n",
    "#ax[1,0].plot(x_axis_val, subsample_fct(mean_lambdas_, time_fct))\n",
    "#ax[1,0].set_title(f\"Lambda\")\n",
    "ax[1,0].plot(x_axis_val, subsample_fct(mean_etas_, time_fct))\n",
    "ax[1,0].set_title(f\"Eta\")\n",
    "\n",
    "ax[1,1].plot(x_axis_val, subsample_fct(mean_alphas_, time_fct))\n",
    "ax[1,1].set_title(f\"Alpha\")\n",
    "\n",
    "ax[1,2].plot(x_axis_val, subsample_fct(losses, time_fct))\n",
    "ax[1,2].set_title(f\"Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_qs_subsample[-1])\n",
    "print(mean_epsilons_[-1])\n",
    "print(Phi_(torch.tensor(mean_qs_subsample[-1]), grad_data, p_torch, dist_Tp_Tq, 0.001, 100))\n",
    "print(erm(torch.tensor(mean_qs_subsample[-1]).unsqueeze(0), torch_all_ranks, n=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3245d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_torch)\n",
    "print(q2)\n",
    "print(torch.norm(p_torch.squeeze(0)-q2, 1))\n",
    "print(erm(q2.unsqueeze(0), torch_all_ranks, n=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72388e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_smooth = phis_[-1] #monte_carlo_phi(p_torch, q2, dist_Tp_Tq, nb_simu=100, std_dev_=0.0001).item()\n",
    "myres = delta - np.asarray(alphas_)*np.asarray(epsilons_) + np.asarray(alphas_)*torch.norm(p_torch.squeeze(0)-q2, 1).item() - phi_smooth\n",
    "\n",
    "plt.plot(myres)\n",
    "plt.show()\n",
    "\n",
    "print(myres[-1])\n",
    "print((np.asarray(alphas_)*np.asarray(epsilons_))[-1])\n",
    "print((np.asarray(alphas_)*torch.norm(p_torch.squeeze(0)-q2, 1).item())[-1])\n",
    "print(f\"phi = {dist_Tp_Tq(p_torch, q2.unsqueeze(0))} vs phi_smooth = {phi_smooth}\")\n",
    "print(q2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6863c9a",
   "metadata": {},
   "source": [
    "# Family of distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e1643b",
   "metadata": {},
   "source": [
    "The Plackett-Luce model is a parametric family of distributions over permutations:\n",
    "$$\n",
    "\\mathbb{P}\\left(\\sigma | w\\right) = \\prod_{r=1}^n \\frac{w_{\\sigma(r)}}{\\sum_{k\\geq r}w_{\\sigma(k)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_plackett_luce(w, all_ranks, n_items=4):\n",
    "    list_proba = list()\n",
    "\n",
    "    for sigma in all_ranks:\n",
    "        val_ = list()\n",
    "        for r in range(n_items):\n",
    "            num_ = w[sigma[r]]\n",
    "            denom_ = 0\n",
    "            for s in range(r, n_items):\n",
    "                v_ = w[sigma[s]]\n",
    "                denom_ += v_\n",
    "            val_.append(num_/denom_)\n",
    "        proba_ = np.prod(val_)\n",
    "        list_proba.append(proba_)\n",
    "    return np.matrix(list_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810438c",
   "metadata": {},
   "source": [
    "# Statistics under study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f42f8f1",
   "metadata": {},
   "source": [
    "### MaxPair\n",
    "\n",
    "Todo: add a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _maxpair(P, threshold = 0.):\n",
    "    s = torch.sum(1*(P > 0.5)+1/2*(P == 0.5), axis=1)\n",
    "    sigma = torch.argsort(-s)\n",
    "    M = P[np.ix_(sigma,sigma)]\n",
    "    \n",
    "    idxs = torch.argwhere(M>1/2+threshold)\n",
    "    for idx in idxs:\n",
    "        M[:idx[0]+1,idx[1]] = 1\n",
    "        M[idx[1],:idx[0]+1] = 0\n",
    "\n",
    "    m = torch.max(torch.abs(M-0.5)*(M!=0.5)*(torch.abs(M-0.5) <= threshold))\n",
    "    while m > 0:\n",
    "        i,j = torch.argwhere(np.abs(M-0.5) == m)[0,0], torch.argwhere(np.abs(M-0.5) == m)[0,1]\n",
    "        if i <= j:\n",
    "            idx_tomerge1, idx_tomerge2 = i, j+1\n",
    "        elif i > j:\n",
    "            idx_tomerge1, idx_tomerge2 = j, i+1\n",
    "        M = torch_merge_idx(M, torch.arange(idx_tomerge1,idx_tomerge2))\n",
    "        m = torch.max(np.abs(M - 0.5) * (M != 0.5) * (torch.abs(M - 0.5) <= threshold))\n",
    "    return M[np.ix_(torch.argsort(sigma), torch.argsort(sigma))]\n",
    "\n",
    "def maxpair(p, torch_all_ranks, n=4, threshold = 0.):\n",
    "    P = pairwise_matrix(p, torch_all_ranks=torch_all_ranks, n=n)\n",
    "    return _maxpair(P, threshold = threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655dd40",
   "metadata": {},
   "source": [
    "### Merge\n",
    "\n",
    "Todo: add a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb01abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_merge_idx(M, idx):\n",
    "    # i,j \\notin idx -> P_ij = M_ij\n",
    "    # i \\in idx, j \\notin idx -> P_ij = \\max_{k\\in idx} M_kj\n",
    "    # i \\notin idx, j \\in idx -> P_ij = \\max_{k\\in idx} M_ik\n",
    "    # i,j \\in idx -> P_ij = 0.5\n",
    "    P = M\n",
    "    for i in torch.arange(M.shape[0]):\n",
    "        m = torch.max(M[i, idx])\n",
    "        for j in idx:\n",
    "            P[i,j] = m\n",
    "    for j in torch.arange(M.shape[0]):\n",
    "        m = torch.max(M[idx, j])\n",
    "        for i in idx:\n",
    "            P[i,j] = m\n",
    "    for i in idx:\n",
    "        for j in idx:\n",
    "            P[i,j] = 0.5\n",
    "    PTRIU = torch.triu(P, 0)\n",
    "    P = PTRIU + torch.tril(1 - PTRIU.T, -1)\n",
    "    return P\n",
    "\n",
    "def merge(p, torch_all_ranks, threshold=0, n=4):\n",
    "    P = pairwise_matrix(p, torch_all_ranks=torch_all_ranks, n=n)\n",
    "    cont = True\n",
    "    while cont:\n",
    "        P_mod = torch.abs(torch.triu(P,1)-torch.tensor(0.5))\n",
    "        m = torch.min(torch.abs(torch.triu(P,1)-torch.tensor(0.5)))\n",
    "        if m == 0.0 or m > threshold:\n",
    "            cont = False\n",
    "        else:\n",
    "            idxs = torch.argwhere(P_mod == m)\n",
    "            idxs = idxs.reshape(torch.prod(torch.tensor(idxs.shape)))\n",
    "            P = torch_merge_idx(P, idxs)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f13fcf",
   "metadata": {},
   "source": [
    "### ERM\n",
    "\n",
    "Todo: add a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d12156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def erm(p, torch_all_ranks, n=4):\n",
    "    P = pairwise_matrix(p, torch_all_ranks=torch_all_ranks, n=n)\n",
    "    return torch.round(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347c53f",
   "metadata": {},
   "source": [
    "### Depth\n",
    "\n",
    "Todo: add a description\n",
    "\n",
    "Only available for n=4 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efa042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendall_tau(sigma1, sigma2):\n",
    "    n = sigma1.size()[-1]\n",
    "    sigma1_inv = torch.argsort(sigma1, dim=-1)\n",
    "    sigma2_inv = torch.argsort(sigma2, dim=-1)\n",
    "    sigma1_pairs = (sigma1_inv.unsqueeze(dim=-1) > sigma1_inv.unsqueeze(dim=-2)).float()\n",
    "    sigma2_pairs = (sigma2_inv.unsqueeze(dim=-1) > sigma2_inv.unsqueeze(dim=-2)).float()\n",
    "    return torch.sum(torch.abs(sigma1_pairs-sigma2_pairs), dim=[-2,-1]).double()/2 #/(n*(n-1))\n",
    "\n",
    "def kendall_matrix(torch_all_ranks):\n",
    "    K = torch.zeros(len(torch_all_ranks),len(torch_all_ranks))\n",
    "    for i, rank1 in enumerate(torch_all_ranks):\n",
    "        for j, rank2 in enumerate(torch_all_ranks):\n",
    "            K[i,j] = kendall_tau(rank1,rank2)\n",
    "    return K.double()\n",
    "\n",
    "def get_all_buckets(torch_all_ranks, n=4):\n",
    "    list_bucket = list()\n",
    "\n",
    "    for rank1 in torch_all_ranks:\n",
    "        for rank2 in torch_all_ranks:\n",
    "            if kendall_tau(rank1,rank2) == 1.0:\n",
    "                list_bucket.append( [rank1,rank2] )\n",
    "    \n",
    "    for i in np.arange(n):\n",
    "        temp_ranks = list()\n",
    "        for rank in torch_all_ranks:\n",
    "            if rank[3] == i:\n",
    "                temp_ranks.append(rank)\n",
    "        list_bucket.append(temp_ranks)\n",
    "        \n",
    "    list_bucket.append([torch.tensor([0,1,2,3]), torch.tensor([0,1,3,2]),torch.tensor([1,0,2,3]),torch.tensor([1,0,3,2])])\n",
    "    list_bucket.append([torch.tensor([0,2,1,3]), torch.tensor([0,2,3,1]),torch.tensor([2,0,1,3]),torch.tensor([2,0,3,1])])\n",
    "    list_bucket.append([torch.tensor([0,3,1,2]), torch.tensor([0,3,2,1]),torch.tensor([3,0,1,2]),torch.tensor([3,0,2,1])])\n",
    "    list_bucket.append([torch.tensor([1,2,0,3]), torch.tensor([1,2,3,0]),torch.tensor([2,1,0,3]),torch.tensor([2,1,3,0])]) \n",
    "    list_bucket.append([torch.tensor([1,3,0,2]), torch.tensor([1,3,2,0]),torch.tensor([3,1,0,2]),torch.tensor([3,1,2,0])])\n",
    "    list_bucket.append([torch.tensor([2,3,0,1]), torch.tensor([2,3,1,0]),torch.tensor([3,2,0,1]),torch.tensor([3,2,1,0])])\n",
    "\n",
    "    list_bucket.append(torch_all_ranks)\n",
    "    for rank in torch_all_ranks:\n",
    "        list_bucket.append(list([rank]))\n",
    "    \n",
    "    return list_bucket\n",
    "\n",
    "def bucket_distrib(torch_all_ranks, list_bucket):\n",
    "    l_ = list()\n",
    "    div = len(list_bucket)\n",
    "    for a_ in torch_all_ranks:\n",
    "        count = 0\n",
    "        for b_ in list_bucket:\n",
    "            count += 1\n",
    "            if (b_ == a_).all():\n",
    "                l_.append(1.0/div)\n",
    "                count += 1\n",
    "                continue\n",
    "        if count == div:\n",
    "            l_.append(0)\n",
    "    return torch.tensor(l_).double()\n",
    "\n",
    "def depth_metric_optim(p, K, list_bucket, torch_all_ranks, norm=\"l1\", printer=False):\n",
    "    n_ranks_to_test = len(list_bucket)\n",
    "    val = torch.inf\n",
    "    for i, bucket in enumerate(list_bucket):\n",
    "        q = bucket_distrib(torch_all_ranks, bucket)\n",
    "        if norm == \"l1\":\n",
    "            val_ = torch.norm(torch.matmul(p.double(),K) - torch.matmul(q.double(),K), 1)\n",
    "        elif norm == \"l2\":\n",
    "            val_ = torch.norm(torch.matmul(p.double(),K) - torch.matmul(q.double(),K), 2)**2\n",
    "        if printer:\n",
    "            print(f\"val: {val_} for {bucket}\")\n",
    "        if val_ <= val:\n",
    "            best_distrib = q\n",
    "            val = val_\n",
    "    return best_distrib, val\n",
    "\n",
    "def depth(p, torch_all_ranks, norm=\"l1\", printer=False):\n",
    "    list_bucket = get_all_buckets(torch_all_ranks, n=4)\n",
    "    K = kendall_matrix(torch_all_ranks)\n",
    "    q, val = depth_metric_optim(p, K, list_bucket, torch_all_ranks, norm=norm, printer=printer)\n",
    "    Q = pairwise_matrix(q.unsqueeze(0), torch_all_ranks=torch_all_ranks, n=4)\n",
    "    return Q  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43ff836",
   "metadata": {},
   "source": [
    "### Wasserstein\n",
    "\n",
    "Todo: add a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d7e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from math import factorial\n",
    "\n",
    "def wasserstein_dist(p, q, K, n=4):\n",
    "    nsize = factorial(n)\n",
    "    K_bis = K.view(nsize*nsize)\n",
    "    \n",
    "    for i in range(nsize):\n",
    "        A_ = [0]*nsize*i + [1]*nsize + [0]*nsize*(nsize-i-1)\n",
    "        if i == 0:\n",
    "            A = torch.tensor(A_).reshape(1,nsize*nsize)\n",
    "        else:\n",
    "            A = torch.cat((A, torch.tensor(A_).reshape(1,nsize*nsize)))\n",
    "    for i in range(nsize):\n",
    "        A_ = ([0]*i + [1] + [0]*(nsize-i-1))*nsize\n",
    "        A = torch.cat((A, torch.tensor(A_).reshape(1,nsize*nsize)))\n",
    "    \n",
    "    b = torch.cat((p, q), 1)\n",
    "    \n",
    "    optim_val = optimize.linprog(K_bis, A_eq=A, b_eq=b, bounds=(0,1))\n",
    "    return optim_val.fun\n",
    "\n",
    "def wasserstein_(myp, torch_all_ranks, n=4, printer=False):\n",
    "    list_bucket = get_all_buckets(torch_all_ranks, n=4)\n",
    "    K = kendall_matrix(torch_all_ranks)\n",
    "    val = torch.inf\n",
    "    for i, bucket in enumerate(list_bucket):\n",
    "        q = bucket_distrib(torch_all_ranks, bucket)\n",
    "        if len(q.shape) < 2:\n",
    "            q = q.unsqueeze(0)\n",
    "        val_ = wasserstein_dist(myp, q, K, n=n)\n",
    "        #if printer:\n",
    "        print(f\"WASS i = {i} --> val: {val_} for {bucket} / q = {q} and p = {myp}\")\n",
    "        if val_ <= val:\n",
    "            best_distrib = q\n",
    "            val = val_\n",
    "    print(f\"END WASSERSTEIN_ with {val_} and {best_distrib}\")\n",
    "    return best_distrib, val\n",
    "\n",
    "def wasserstein(myp, torch_all_ranks, n=4):\n",
    "    print(f\"CALL TO WASSERSTEIN\")\n",
    "    q, val = wasserstein_(myp, torch_all_ranks, n=n, printer=False)\n",
    "    Q = pairwise_matrix(q, torch_all_ranks=torch_all_ranks, n=n)\n",
    "    print(f\"END OF CALL TO WASSERSTEIN\")\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db145753",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b312827",
   "metadata": {},
   "source": [
    "## Checking the converge of the optimization of the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([25, 8, 3, 2.5])\n",
    "p = proba_plackett_luce(w, all_ranks)\n",
    "p_torch = torch.from_numpy(p)\n",
    "\n",
    "torch_all_ranks = torch.from_numpy(np.asarray(all_ranks))\n",
    "P = pairwise_matrix(p_torch, torch_all_ranks, n=4)\n",
    "\n",
    "method_ = maxpair #depth\n",
    "def torch_dist_maxpair(p_torch1, p_torch2, torch_all_ranks, threshold):\n",
    "    R1 = method_(p_torch1, torch_all_ranks, n=4, threshold=threshold)\n",
    "    R2 = method_(p_torch2, torch_all_ranks, n=4, threshold=threshold)\n",
    "    return symmetrized_hausdorff_on_kendall(R1, R2)\n",
    "\n",
    "method = torch_dist_maxpair\n",
    "\n",
    "epochs = 1000\n",
    "threshold = 0.0\n",
    "\n",
    "dist_Tp_Tq = lambda _p,_q: method(_p, _q, torch_all_ranks, threshold=threshold)\n",
    "\n",
    "softplus = torch.nn.Softplus(beta=1, threshold=20)\n",
    "kernel_conv = lambda _s: MultivariateNormal(_s, 0.1*torch.eye(_s.size()[-1])) # $x\\mapsto k_x(.)$\n",
    "smoothed_dist_Tp_Tq = lambda p, q: smooth_pg_2(dist_Tp_Tq, kernel_conv)(p, q)     # $\\phi(.,.)$\n",
    "\n",
    "s_ = p_torch[0,:].detach().clone().to(device).type(default_tensor_type)\n",
    "s_.requires_grad = True\n",
    "optimizer = torch.optim.SGD([s_], lr=0.01, momentum=0.9)\n",
    "scheduler = CyclicLR(optimizer, 0.01, 1.0, step_size_down=50, cycle_momentum=False)\n",
    "\n",
    "losses = []\n",
    "noreglosses = []\n",
    "metrics = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    q = softplus(s_)/torch.sum(softplus(s_))                                      # projection $\\sum = 1$\n",
    "    \n",
    "    kernel_conv = lambda _s: MultivariateNormal(_s, (10/(10+epoch))*torch.eye(_s.size()[-1])) # $x\\mapsto k_x(.)$\n",
    "    smoothed_dist_Tp_Tq = lambda p, q: smooth_pg_2(dist_Tp_Tq, kernel_conv)(p, q)     # $\\phi(.,.)$\n",
    "\n",
    "    loss = -smoothed_dist_Tp_Tq(p_torch, q) + 2.5*(torch.norm(p_torch-q, 1))      # Lagrangian relaxation of the objective\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    losses.append(loss.detach().numpy())\n",
    "    noreglosses.append(smoothed_dist_Tp_Tq(p_torch, q).detach().numpy())\n",
    "    metrics.append(dist_Tp_Tq(p_torch, torch.unsqueeze(q,0)).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f79cb97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def movingaverage(interval, window_size=10):\n",
    "    window= np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(interval, window, 'same')\n",
    "    \n",
    "plt.plot(movingaverage(np.ravel(losses)/100), 'b')\n",
    "plt.plot(movingaverage(-np.ravel(noreglosses)/100), 'r')\n",
    "plt.plot(np.ravel(metrics), 'g')\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8332bd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f6e3e09",
   "metadata": {},
   "source": [
    "After some tests, it seems using a plain SGD with cyclic stepsize is a good way to handle the specific landscape of the objective that alternates between plateau and sharp slopes.\n",
    "\n",
    "**However**, the optimization does not seems satisfactory yet, as highlighted by the following experiments that shows the estimate of $\\tilde \\varepsilon$ as a function of $\\delta$ is not monotonous.\n",
    "\n",
    "**Hypothesis**: As we are looking for the minimal budget $\\varepsilon^*$ that allows to break the statistic $T$ by at least $\\delta$, it means that, for an $\\varepsilon$ close to $\\varepsilon^*$, most of the feasible attacks $\\tilde q$ break the statistic $T$ by **strictly less** than $\\delta$. At the limit of $\\varepsilon \\to \\varepsilon^*_+$, only the optimal attack breaks the statistic by $\\delta$, which explains that even assymptotically during the optimization (see figure above), the value of $\\delta$ is not fully stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb995b8c",
   "metadata": {},
   "source": [
    "## Studying the computation of $\n",
    "\\tilde\\varepsilon(\\delta,p,T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d9ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = np.array([25, 8, 3, 2.5])\n",
    "p = proba_plackett_luce(w, all_ranks)\n",
    "p_torch = torch.from_numpy(p)\n",
    "\n",
    "torch_all_ranks = torch.from_numpy(np.asarray(all_ranks))\n",
    "P = pairwise_matrix(p_torch, torch_all_ranks, n=4)\n",
    "dist = \"erm\"\n",
    "\n",
    "def torch_dist(dist, p_torch1, p_torch2, torch_all_ranks, threshold, dist_type_sym=True):\n",
    "    if dist == \"erm\":\n",
    "        R1 = erm(p_torch1, torch_all_ranks, n=4)\n",
    "        R2 = erm(p_torch2, torch_all_ranks, n=4)\n",
    "    elif dist == \"maxpair\":\n",
    "        R1 = maxpair(p_torch1, torch_all_ranks, n=4, threshold=threshold)\n",
    "        R2 = maxpair(p_torch2, torch_all_ranks, n=4, threshold=threshold)\n",
    "    elif dist == \"merge\":\n",
    "        R1 = merge(p_torch1, torch_all_ranks, threshold=threshold, n=4)\n",
    "        R2 = merge(p_torch2, torch_all_ranks, threshold=threshold, n=4)\n",
    "    elif dist == \"depth\":\n",
    "        R1 = depth(p_torch1, torch_all_ranks, norm=\"l1\", printer=False)\n",
    "        R2 = depth(p_torch2, torch_all_ranks, norm=\"l1\", printer=False)\n",
    "    elif dist == \"wasserstein\":\n",
    "        R1 = wasserstein(p_torch1, torch_all_ranks, n=4)\n",
    "        R2 = wasserstein(p_torch2, torch_all_ranks, n=4)\n",
    "    if dist_type_sym:\n",
    "        return symmetrized_hausdorff_on_kendall(R1, R2)\n",
    "    else:\n",
    "        return disymmetrized_hausdorff_on_kendall(R1, R2)\n",
    "\n",
    "method = torch_dist\n",
    "\n",
    "epochs = 4000\n",
    "threshold = 0.0\n",
    "\n",
    "dist_Tp_Tq = lambda _p,_q: method(dist, _p, _q, torch_all_ranks, threshold=threshold)\n",
    "\n",
    "delta = 1.0\n",
    "losses, epsilons, q2 = approximate_breakdown_function(delta, dist_Tp_Tq, p_torch, epochs=epochs, std_dev=0.1, lr=0.01, max_reg=10, maxiter=11, eval_strat=\"smoothed\")\n",
    "#eps = losses[-1]\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "print(\"EPS(DELTA=\"+str(delta)+\"):\", epsilons[-1])\n",
    "\n",
    "#regs = np.linspace(0., 4, 21)\n",
    "#q_list = [torch_optim_attack_relaxed(dist_Tp_Tq, p_torch, reg, epochs, std_dev=10, lr=0.01) for reg in regs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_torch)\n",
    "print(q2)\n",
    "print(torch.norm(p_torch - q2, 1))\n",
    "\n",
    "erm(q2.unsqueeze(0), torch_all_ranks, n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbbc3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_list = [torch.norm(p_torch - torch.unsqueeze(_q,0),1).detach().numpy() for _q in q_list]\n",
    "delta_list = [dist_Tp_Tq(p_torch, torch.unsqueeze(_q,0)) for _q in q_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548f046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f872a2d7",
   "metadata": {},
   "source": [
    "**SANITY CHECK:** The attack budget $\\varepsilon$ is a (decreasing) function of the Lagrange parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regs, eps_list)\n",
    "plt.title(f\"Attack budget (eps) as a function of regularization\")\n",
    "plt.xlabel(f\"Regularization\")\n",
    "plt.ylabel(f\"Attack budget (eps)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9d75c",
   "metadata": {},
   "source": [
    "**SANITY CHECK:** The attack is stronger for lower Lagrange variable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6af19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regs, delta_list)\n",
    "plt.title(f\"Loss as a function of regularization\")\n",
    "plt.xlabel(f\"Regularization\")\n",
    "plt.ylabel(f\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574d239",
   "metadata": {},
   "source": [
    "**RESULT:** $\\varepsilon^*(\\delta,p,T)$ (lower is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7ae63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(delta_list, eps_list)\n",
    "plt.title(f\"Loss as a function of attack budget (eps)\")\n",
    "plt.xlabel(f\"Attack budget (eps)\")\n",
    "plt.ylabel(f\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdcae42",
   "metadata": {},
   "source": [
    "## Performance vs Robustness profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6406122a",
   "metadata": {},
   "source": [
    "**WARNING** The following cell takes a lot of time. It's still not finalized as the computation of $\\tilde\\varepsilon$ is not yet reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b324a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = \"erm\"\n",
    "w = np.array([25, 8, 3, 2.5])\n",
    "delta = 1\n",
    "epochs = 100 #150000 #True val is 150000\n",
    "thresholds_ = np.linspace(0., 0.5, 3)\n",
    "\n",
    "def launch_exp(dist, w, delta, thresholds_, epochs):\n",
    "    p = proba_plackett_luce(w, all_ranks)\n",
    "    p_torch = torch.from_numpy(p)\n",
    "    torch_all_ranks = torch.from_numpy(np.asarray(all_ranks))\n",
    "    P = pairwise_matrix(p_torch, torch_all_ranks, n=4)\n",
    "\n",
    "    def torch_dist(dist, p_torch1, p_torch2, torch_all_ranks, threshold, dist_type_sym=True):\n",
    "        if dist == \"erm\":\n",
    "            R1 = erm(p_torch1, torch_all_ranks, n=4)\n",
    "            R2 = erm(p_torch2, torch_all_ranks, n=4)\n",
    "        elif dist == \"maxpair\":\n",
    "            R1 = maxpair(p_torch1, torch_all_ranks, n=4, threshold=threshold)\n",
    "            R2 = maxpair(p_torch2, torch_all_ranks, n=4, threshold=threshold)\n",
    "        elif dist == \"merge\":\n",
    "            R1 = merge(p_torch1, torch_all_ranks, threshold=threshold, n=4)\n",
    "            R2 = merge(p_torch2, torch_all_ranks, threshold=threshold, n=4)\n",
    "        elif dist == \"depth\":\n",
    "            R1 = depth(p_torch1, torch_all_ranks, norm=\"l1\", printer=False)\n",
    "            R2 = depth(p_torch2, torch_all_ranks, norm=\"l1\", printer=False)\n",
    "        elif dist == \"wasserstein\":\n",
    "            R1 = wasserstein(p_torch1, torch_all_ranks, n=4)\n",
    "            R2 = wasserstein(p_torch2, torch_all_ranks, n=4)\n",
    "        if dist_type_sym:\n",
    "            return symmetrized_hausdorff_on_kendall(R1, R2)\n",
    "        else:\n",
    "            return disymmetrized_hausdorff_on_kendall(R1, R2)\n",
    "\n",
    "    eps_list1 = []\n",
    "    eps_list2 = []\n",
    "    perf_list = []\n",
    "    if dist in [\"erm\", \"depth\", \"wasserstein\"]:\n",
    "        thresholds = [0]\n",
    "    elif dist in [\"maxpair\", \"merge\"]:\n",
    "        thresholds = thresholds_\n",
    "        \n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\n \\t EXP THRESHOLD {threshold} \\n \\n \\n\")\n",
    "        dist_Tp_Tq = lambda _p,_q: torch_dist(dist, _p, _q, torch_all_ranks, threshold=threshold)\n",
    "        losses, qs_, epsilons, etas, alphas, s_, mean_qs, mean_epsilons, mean_etas, mean_alphas = approximate_breakdown_function(delta-1e-6, dist_Tp_Tq, p_torch, epochs=epochs, std_dev=0.0001, lr=0.01, maxiter=21, eval_strat=\"smoothed\")\n",
    "        q2 = mean_qs[-1]\n",
    "        eps_list1.append(mean_epsilons[-1])\n",
    "        eps_list2.append(torch.norm(p_torch - torch.tensor(q2).unsqueeze(0), 1))\n",
    "        if dist == \"erm\":\n",
    "            Ptilde = erm(p_torch, torch_all_ranks, n=4)\n",
    "        elif dist == \"maxpair\":\n",
    "            Ptilde = maxpair(p_torch, torch_all_ranks, n=4, threshold=threshold)\n",
    "        elif dist == \"merge\":\n",
    "            Ptilde = merge(p_torch, torch_all_ranks, threshold=threshold, n=4)\n",
    "        elif dist == \"depth\":\n",
    "            Ptilde = depth(p_torch, torch_all_ranks, norm=\"l1\", printer=False)\n",
    "        elif dist == \"wasserstein\":\n",
    "            Ptilde = wasserstein(p_torch, torch_all_ranks, n=4)\n",
    "        exp_kendall = expected_kendall(Ptilde, P).detach().item()\n",
    "        perf_list.append(exp_kendall)\n",
    "    \n",
    "    return perf_list, eps_list1, eps_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c3503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERM\n",
    "dist = \"erm\"\n",
    "perf_list_erm, eps_list_erm1, eps_list_erm2 = launch_exp(dist, w, delta, thresholds_, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"perf_list = {perf_list_erm} and eps_list1 = {eps_list_erm1} and eps_list2 = {eps_list_erm2}\")\n",
    "\n",
    "plt.plot(perf_list_erm, eps_list_erm1, '*')\n",
    "plt.plot(perf_list_erm, eps_list_erm2, '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd8532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth\n",
    "dist = \"depth\"\n",
    "perf_list_depth, eps_list_depth1, eps_list_depth2 = launch_exp(dist, w, delta, thresholds_, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"perf_list = {perf_list_depth} and eps_list = {eps_list_depth}\")\n",
    "plt.plot(perf_list_depth, eps_list_depth1, '*')\n",
    "plt.plot(perf_list_depth, eps_list_depth2, '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a49236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maxpair\n",
    "dist = \"maxpair\"\n",
    "perf_list_maxpair, eps_list_maxpair1, eps_list_maxpair2 = launch_exp(dist, w, delta, thresholds_, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"perf_list = {perf_list_maxpair} and eps_list1 = {eps_list_maxpair1} and eps_list2 = {eps_list_maxpair2}\")\n",
    "plt.plot(perf_list_maxpair, eps_list_maxpair1, '*')\n",
    "plt.plot(perf_list_maxpair, eps_list_maxpair2, '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806df6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge\n",
    "dist = \"merge\"\n",
    "perf_list_merge, eps_list_merge1, eps_list_merge2 = launch_exp(dist, w, delta, thresholds_, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"perf_list = {perf_list_merge} and eps_list = {eps_list_merge}\")\n",
    "plt.plot(perf_list_merge, eps_list_merge1, '*')\n",
    "plt.plot(perf_list_merge, eps_list_merge2, '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fceb62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wasserstein\n",
    "dist = \"wasserstein\"\n",
    "perf_list_wasserstein, eps_list_wasserstein1, eps_list_wasserstein2 = launch_exp(dist, w, delta, thresholds_, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5664ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"perf_list = {perf_list_wasserstein} and eps_list = {eps_list_wasserstein}\")\n",
    "plt.plot(perf_list_wasserstein, eps_list_wasserstein1, '*')\n",
    "plt.plot(perf_list_wasserstein, eps_list_wasserstein2, '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b90528",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = kendall_matrix(torch_all_ranks)\n",
    "p = torch.tensor([[0.0369, 0.0388, 0.0497, 0.0311, 0.0455, 0.0496, 0.0381, 0.0308, 0.0266,\n",
    "         0.0475, 0.0370, 0.0599, 0.0317, 0.0472, 0.0301, 0.0499, 0.0358, 0.0273,\n",
    "         0.0548, 0.0422, 0.0442, 0.0240, 0.0388, 0.0448]])\n",
    "print(torch.sum(p))\n",
    "q = torch.tensor([[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], dtype=torch.float64)\n",
    "val_ = wasserstein_dist(p, q, K, n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f102490",
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(perf_list_erm, eps_list_erm, 'x')\n",
    "plt.plot(perf_list_merge, eps_list_merge, 'o')\n",
    "plt.plot(perf_list_maxpair, eps_list_maxpair, '*')\n",
    "plt.plot(perf_list_depth, eps_list_depth, 's')\n",
    "plt.xlabel(\"Performance (loss)\")\n",
    "plt.ylabel(\"Robustness (bkdwn pts)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aa7f1a",
   "metadata": {},
   "source": [
    "# UNUSED CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e062143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendall_tau_numpy(y, sigma):\n",
    "    y = torch.from_numpy(y)\n",
    "    sigma = torch.from_numpy(sigma)\n",
    "    n = sigma.size()[-1]\n",
    "    sigma_inv = torch.argsort(sigma, dim=-1)\n",
    "    y_inv = torch.argsort(y, dim=-1)\n",
    "    sigma_pairs = (sigma_inv.unsqueeze(dim=-1) > sigma_inv.unsqueeze(dim=-2)).float()\n",
    "    y_pairs = (y_inv.unsqueeze(dim=-1) > y_inv.unsqueeze(dim=-2)).float()\n",
    "    return torch.sum(torch.abs(sigma_pairs-y_pairs), dim=[-2,-1])/2 #/(n*(n-1))\n",
    "\n",
    "def kendall_matrix(all_ranks):\n",
    "    K = np.zeros((len(all_ranks),len(all_ranks)))\n",
    "    for i, rank1 in enumerate(all_ranks):\n",
    "        for j, rank2 in enumerate(all_ranks):\n",
    "            K[i,j] = kendall_tau_numpy(rank1,rank2)\n",
    "    return K\n",
    "\n",
    "def kendall_tau(y, sigma, norm=False):\n",
    "    \"\"\"\n",
    "    Kendall Tau distance\n",
    "    Permutations follow the convention rank -> item_id\n",
    "    :param torch.Tensor y: permutation\n",
    "    :param torch.Tensor sigma: permutation\n",
    "    :return: value of the metric\n",
    "    :rtype: torch.Tensor\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        n = sigma.size()[-1]\n",
    "        sigma_inv = torch.argsort(sigma, dim=-1).to(device).type(default_tensor_type)\n",
    "        y_inv = torch.argsort(y, dim=-1).to(device).type(default_tensor_type)\n",
    "        sigma_pairs = (sigma_inv.unsqueeze(dim=-1) > sigma_inv.unsqueeze(dim=-2)).to(device).type(default_tensor_type)\n",
    "        y_pairs = (y_inv.unsqueeze(dim=-1) > y_inv.unsqueeze(dim=-2)).to(device).type(default_tensor_type)\n",
    "        res = torch.sum(torch.abs(sigma_pairs-y_pairs), dim=[-2,-1])\n",
    "        if norm:\n",
    "            return res/(n*(n-1))\n",
    "        else:\n",
    "            return res/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = kendall_matrix(all_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3978651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_buckets(all_ranks, n_items=4):\n",
    "    list_bucket = list()\n",
    "\n",
    "    for rank1 in all_ranks:\n",
    "        for rank2 in all_ranks:\n",
    "            if kendall_tau_numpy(rank1,rank2) == 1.0:\n",
    "                list_bucket.append( [rank1,rank2] )\n",
    "    \n",
    "    for i in np.arange(4):\n",
    "        temp_ranks = list()\n",
    "        for rank in all_ranks:\n",
    "            if rank[3] == i:\n",
    "                temp_ranks.append(rank)\n",
    "        list_bucket.append(temp_ranks)\n",
    "        \n",
    "    list_bucket.append([np.array([0,1,2,3]), np.array([0,1,3,2]),np.array([1,0,2,3]),np.array([1,0,3,2])])\n",
    "    list_bucket.append([np.array([0,2,1,3]), np.array([0,2,3,1]),np.array([2,0,1,3]),np.array([2,0,3,1])])\n",
    "    list_bucket.append([np.array([0,3,1,2]), np.array([0,3,2,1]),np.array([3,0,1,2]),np.array([3,0,2,1])])\n",
    "    list_bucket.append([np.array([1,2,0,3]), np.array([1,2,3,0]),np.array([2,1,0,3]),np.array([2,1,3,0])]) \n",
    "    list_bucket.append([np.array([1,3,0,2]), np.array([1,3,2,0]),np.array([3,1,0,2]),np.array([3,1,2,0])])\n",
    "    list_bucket.append([np.array([2,3,0,1]), np.array([2,3,1,0]),np.array([3,2,0,1]),np.array([3,2,1,0])])\n",
    "\n",
    "    list_bucket.append(all_ranks)\n",
    "    for rank in all_ranks:\n",
    "        list_bucket.append(list([rank]))\n",
    "    \n",
    "    return list_bucket\n",
    "\n",
    "list_bucket = get_all_buckets(all_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b72fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_distrib(all_ranks, list_bucket):\n",
    "    l_ = list()\n",
    "    div = len(list_bucket)\n",
    "    for a_ in all_ranks:\n",
    "        count = 0\n",
    "        for b_ in list_bucket:\n",
    "            count += 1\n",
    "            if (b_ == a_).all():\n",
    "                l_.append(1.0/div)\n",
    "                count += 1\n",
    "                continue\n",
    "        if count == div:\n",
    "            l_.append(0)\n",
    "    return np.matrix(l_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
